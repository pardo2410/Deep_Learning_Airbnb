# -*- coding: utf-8 -*-
"""Practica_Deep_Learning-Juan_David_Pardo_Pinzon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jY2gkAiZPXZBVhRdxswx60CIX7hBLagE

# Práctica de deep learning utilizando la base de datos de Airbnb

Juan David Pardo P.

#Objetivo

El objetivo de la práctica final del módulo de Deep Learning es abordar un problema del mundo real utilizando las técnicas enseñadas en clase. Específicamente, se busca predecir el precio de las habitaciones de AirBnb utilizando todas las características disponibles en el dataset.

El propósito principal no es lograr un sistema con una precisión extremadamente alta, sino más bien que los estudiantes combinen diferentes tipos de características (numéricas, texto, imágenes, etc.) y expliquen cómo lo han hecho.

En el siguiente enlace, se proporciona un ejemplo de cómo se pueden combinar diferentes tipos de características utilizando una red neuronal:

[Keras Multiple Inputs and Mixed Data](https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/)

El conjunto de datos seleccionado se obtuvo de Airbnb mediante técnicas de scraping. Recomiendo utilizar el extracto "Only the 14780 selected records", ya que reduce el tiempo de ejecución y evita problemas de memoria en equipos con menos recursos.

# Instalación e Importación de librerías.
"""

pip install -U scikit-learn

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

# #Librerías carga de imágenes
from typing import Optional, Union
from tqdm import tqdm
import imageio.v3 as io
import cv2
import seaborn as sns

#Matplolib Settings
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
# %matplotlib inline

# División datos test y entrenamiento
from sklearn.model_selection import train_test_split

# #Estadarización/Normalización
from sklearn.preprocessing import scale
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

# #Tensorflow Keras
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.constraints import max_norm
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Flatten
from tensorflow.keras.regularizers import L1
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.layers import Input

"""# Descarga y preprocesado de los datos

Descargar los datos, almacenar las imágenes y eliminar filas del CSV para los cuales no se haya descargado la imagen.
"""

!wget -O "airbnb-listings.csv" "https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B"

# Validamos la descarga del dataset
!ls -lah

#Leemos los datos y quitamos precios nulos
data = pd.read_csv('airbnb-listings.csv', sep = ';')
data.head(3).T

"""Se valida el tipo de datos con el que trabajara en dataset."""

# Obtiene el valor actual del número máximo de filas
max_rows = pd.get_option('display.max_rows')

# Imprime el valor actual del número máximo de filas
print(f'Número máximo de filas: {max_rows}')

# Set_option para mostrar todas las filas
pd.set_option('display.max_rows', None)

# Muestra los tipos de datos de todas las columnas
data.info()

# Validando el numero de columnas y filas del dataset
data.shape

"""# Se llevará a cabo una limpieza preliminar antes de procesar los datos.

En los análisis de los módulos anteriores se observó que la gran mayoría de los datos están distribuidos en España.
"""

countries = data['Country'].value_counts()
print(countries)

"""Tras realizar un análisis más detallado de los datos de España, se descubrió que el 94.33% de los registros provienen de la ciudad de Madrid. Por esta razón, se ha tomado la decisión de concentrar todo el proceso de análisis en dicha ciudad."""

import pandas as pd

# Filtrar solo las filas correspondientes a Spain
spain_data = data[data['Country'] == 'Spain']

# Obtener el número de registros por ciudad en Spain
city_counts = spain_data['City'].value_counts()

# Mostrar la lista de número de registros por ciudad
print("Número de registros de Spain por ciudad:")
print(city_counts)

"""Se identificó la necesidad de realizar ajustes en varios registros del conjunto de datos para lograr una normalización completa de todos los registros correspondientes a Madrid"""

# Definir los nombres incorrectos que quieres reemplazar por 'Madrid'
incorrect_names = ['Chueca, Madrid','é©¬å¾·é','las matas  madrid','Comunidad de Madrid','Centro, Madrid','Delicias-Madrid','Aravaca (Madrid)','Madrid, Comunidad de Madrid, ES', 'madrid', 'MADRID', 'Madri', 'Madid','Madrid, Vallecas (FontarrÃ³n)','Madrid, Comunidad de Madrid, ESPANA']

# Filtrar solo las filas correspondientes a 'Spain'
spain_data_fix = data[data['Country'] == 'Spain']

# Utilizar .loc para evitar el SettingWithCopyWarning
spain_data_fix.loc[:, 'City'] = spain_data_fix['City'].replace(incorrect_names, 'Madrid')

# Mostrar la lista actualizada de registros por ciudad en Spain
city_counts = spain_data_fix['City'].value_counts()
print("Número de registros de Spain por ciudad (actualizado):")
print(city_counts)

"""Se procede a eliminar todas las filas que no corresponden a Madrid con el objetivo de centrar nuestro análisis en la ciudad que presenta la mayor cantidad de datos disponibles."""

def filter_madrid(data_frame):
    # Filtrar solo las filas correspondientes a 'Spain'
    spain_data_fix = data_frame[data_frame['Country'] == 'Spain']

    # Filtrar las filas donde 'City' sea 'Madrid'
    madrid_data_fix = spain_data_fix[spain_data_fix['City'] == 'Madrid']

    return madrid_data_fix

# Llama a la función para filtrar Madrid y almacenar en airbnb_df_madrid
airbnb_df_madrid = filter_madrid(spain_data)
# Establecer la opción para mostrar todas las columnas
pd.set_option('display.max_columns', None)

airbnb_df_madrid.head(3).T

"""Procedemos a validar que los cambios se hayan efectuado exitosamente. Se observa que en la variable 'Country' solo aparece 'Spain', y en la variable 'City' solo tenemos registros asociados a Madrid. Confirmamos entonces que el ajuste se aplicó de manera correcta."""

countries = airbnb_df_madrid['Country'].value_counts()
print(countries)

# Filtrar solo las filas correspondientes a Spain
spain_data = airbnb_df_madrid[airbnb_df_madrid['Country'] == 'Spain']
# Obtener el número de registros por ciudad en Spain
city_counts = spain_data['City'].value_counts()
# Mostrar la lista de número de registros por ciudad
print("Número de registros de Spain por ciudad:")
print(city_counts)

"""Ahora procedemos a validar la variable 'Zipcode'."""

# Llama los Zipcode y los cuenta
print ("Madrid muestra los siguientes valores en 'Zipcode':\n",data.loc[data['City'] == 'Madrid', 'Zipcode'].value_counts())
# Llama el tipo de dato
df_zipcode = data.loc[data['City'] == 'Madrid', 'Zipcode'].value_counts()
# Cuenta el numero de NAN en la columna
print("\n Tambien se encuentran ", data.loc[data['City'] == 'Madrid', 'Zipcode'].isna().sum(), " NAN con lo que se comprueba el total: \n")
# Cuenta el numero de valores unicos
df_zipcode.shape

"""Vamos a ajustar la columna de Zipcode"""

# Ajustes de códigos postales
corrections = {'28002\n28002': 28002, '2802\n28012': 28012, '08015': 28015, '28051\n28051': 28051, 'Madrid 28004': 28004, 25008: 28008,
                2805: 28015, 280013: 28013, 2015: 28015, 2804: 28004, 29012: 28012, 27013: 28013, 2815: 28015, 10100: 28100, 20013: 28013,
                20126: 28126, 27004: 28004, 29230: 28830
               }

# Ajustar códigos postales
airbnb_df_madrid['Zipcode'] = airbnb_df_madrid['Zipcode'].replace(corrections)
# Eliminar filas con valores no válidos
invalid_zipcodes = ['-', 28, 3430, 7320]
airbnb_df_madrid = airbnb_df_madrid[~airbnb_df_madrid['Zipcode'].isin(invalid_zipcodes)]
# Eliminar filas con valores NaN en la columna 'Zipcode'
airbnb_df_madrid = airbnb_df_madrid.dropna(subset=['Zipcode'])
# Convertir a tipo entero
airbnb_df_madrid['Zipcode'] = airbnb_df_madrid['Zipcode'].astype(int)
# Filtrar códigos postales válidos para Madrid
valid_zipcodes = airbnb_df_madrid[(airbnb_df_madrid['Zipcode'] >= 28001) & (airbnb_df_madrid['Zipcode'] <= 28999)]
# Validar que los códigos postales sean únicos
unique_zipcodes = np.unique(valid_zipcodes['Zipcode'])
# Mostrar códigos postales únicos
print(unique_zipcodes)

"""Finalmente, llevamos a cabo un análisis de las variables que presentan un mayor número de valores faltantes en el conjunto de datos, con el objetivo de garantizar la inclusión de las variables más apropiadas en nuestro análisis.

Se calcula el número de NaN contenido en el dataset y su importancia porcentual:
"""

# Contar el número de NaN por columnas
nan_counts = airbnb_df_madrid.isnull().sum()
# Calcular el porcentaje de NaN por columnas
nan_percentage = (nan_counts / len(airbnb_df_madrid)) * 100
# Crear un nuevo DataFrame con la información de NaN y porcentaje
nan_info = pd.DataFrame({
    'NaN Count': nan_counts,
    'NaN Percentage': nan_percentage
})
# Ordenar de mayor a menor porcentaje
nan_info_sorted = nan_info.sort_values(by='NaN Percentage', ascending=False)
# Imprimir la salida completa ordenada
pd.set_option('display.max_rows', None)
print(nan_info_sorted)

"""Después de llevar a cabo una validación preliminar de los datos, opté por realizar una depuración de la información antes de proceder con la división del conjunto en datos de entrenamiento y prueba. Durante este proceso de validación, observé que no todos los datos son relevantes para el análisis, considerando especialmente la variable objetivo.

Vamos a proceder a eliminar las columnas que no son relevantes para nuestro análisis. El criterio de selección consiste en eliminar aquellas variables que tienen un valor porcentual mayor o igual al 70% de muestras en NaN, exceptuando algunas variables que considero podrían resultar de importancia.
"""

useful_data = [
                'ID', 'Listing Url', 'Scrape ID', 'Last Scraped', 'Summary', 'Space', 'Description',
                'Neighborhood Overview', 'Notes', 'Transit', 'Access', 'Interaction', 'House Rules',
                'Medium Url', 'Picture Url', 'XL Picture Url', 'Host ID', 'Host URL',
                'Host Name', 'Host Location', 'Host Thumbnail Url', 'Host Picture Url',
                'Host Neighbourhood','City', 'Market', 'Country Code', 'Country', 'Has Availability',
                'License', 'Jurisdiction Names','Street','State','Name','Host About','First Review',
                'Last Review', 'Calendar last Scraped', 'Geolocation','Host Acceptance Rate',
                'Square Feet', 'Security Deposit', 'Amenities', 'Availability 30', 'Availability 60',
                'Availability 90', 'Availability 365','Review Scores Rating', 'Review Scores Accuracy',
                'Review Scores Cleanliness', 'Review Scores Checkin', 'Cleaning Fee', 'Host Acceptance Rate',
                'Review Scores Communication', 'Review Scores Location', 'Monthly Price',
                'Review Scores Value', 'Jurisdiction Names', 'Weekly Price'
            ]
# Eliminar las columnas
airbnb_df_madrid.drop(useful_data, axis=1, inplace=True)
# Validar si se eliminaron o no correctamente los datos
if all(column not in airbnb_df_madrid.columns for column in useful_data):
    print("Columnas eliminadas correctamente.")
else:
    print("Columnas no encontradas en el DataFrame.")

"""Volvemos a validar el numero ro de filas con NAN"""

# Contar el número de NaN por columnas
nan_counts = airbnb_df_madrid.isnull().sum()
# Calcular el porcentaje de NaN por columnas
nan_percentage = (nan_counts / len(airbnb_df_madrid)) * 100
# Crear un nuevo DataFrame con la información de NaN y porcentaje
nan_info = pd.DataFrame({
    'NaN Count': nan_counts,
    'NaN Percentage': nan_percentage
})
# Ordenar de mayor a menor porcentaje
nan_info_sorted = nan_info.sort_values(by='NaN Percentage', ascending=False)
# Imprimir la salida completa ordenada
pd.set_option('display.max_rows', None)
print(nan_info_sorted)

"""# Descarga de imagenes

Se valida el tipo de datos de las variables restantes en el dataset.
"""

# Obtiene el valor actual del número máximo de filas
max_rows = pd.get_option('display.max_rows')
# Imprime el valor actual del número máximo de filas
print(f'Número máximo de filas: {max_rows}')
# Se usa set_option para mostrar todas las filas
pd.set_option('display.max_rows', None)
# Muestra los tipos de datos de todas las columnas
airbnb_df_madrid.info()

"""# Descargar y procesado de las imágenes utilizadas para realizar el análisis

La función toma argumentos opcionales como el tamaño del lienzo (canvas), el número de canales (nb_channels) y el número máximo de imágenes para descargar (max_imgs). Luego, inicializa un arreglo de ceros para almacenar las imágenes descargadas y un registro de los índices de las imágenes descargadas. Itera sobre las URLs, descarga y redimensiona las imágenes, y las almacena en el arreglo. Finalmente, devuelve las imágenes descargadas y sus índices correspondientes.
"""

#Aplicamos la funcion para descargar el conjunto de imagenes a procesar
def download_images(paths: list,
                    canvas: tuple = (32, 32),
                    nb_channels: int = 3,
                    max_imgs: Optional[int] = None
                    ) -> tuple:

  n_images = len(paths) if not max_imgs else max_imgs
  images = np.zeros((n_images, canvas[0], canvas[1], nb_channels),
                       dtype=np.uint8)
  downloaded_idxs = []

  for i_img, url in enumerate(tqdm(paths, total=n_images)):
    if i_img >= n_images:
      break
    try:
        img = io.imread(url)
        img = cv2.resize(img, (canvas[0], canvas[1]))
        downloaded_idxs.append(i_img)
        images[i_img] = img
    except (IOError, ValueError) as e:
        pass
  return images[downloaded_idxs], downloaded_idxs

"""Descarga imágenes utilizando las URL proporcionadas en la columna 'Thumbnail Url' del DataFrame 'data' con un límite máximo de 3000 imágenes, convierte las imágenes a valores de punto flotante en el rango [0, 1] y luego imprime la forma de la matriz de imágenes resultante."""

images, downloaded_idxs = download_images(data['Thumbnail Url'], max_imgs=1000)
images = images.astype("float32") / 255.
print(images.shape)

filtered_data = airbnb_df_madrid.iloc[downloaded_idxs]
filtered_data = filtered_data.reset_index(drop=True)
filtered_data.head(3).T

"""Validación y presentación de los datos que se utilizarán para el análisis."""

print("\n Dataset Base:", data.shape)
print("\n Dataset despues de la limpieza preliminar:", airbnb_df_madrid.shape)
print("\n Dataset de imagenes:", images.shape)
print("\n Dataset de datos filtrados en funcion de las imagenes encontradas:",filtered_data.shape)

"""# Guardamos los datos para su posterior uso"""

from google.colab import drive
drive.mount("/content/drive")

np.save("images_airbnb.npy", images)
filtered_data.to_csv("filtered_data.csv", sep=";", index=False)

"""Cargamos los ficheros anteriormente guardados en Google Drive"""

!cp images_airbnb.npy /content/drive/MyDrive/images_airbnb.npy
!cp filtered_data.csv /content/drive/MyDrive/filtered_data.csv
!ls -lah

"""Comprobamos a abrirlos de nuevo"""

saved_data = pd.read_csv("/content/drive/MyDrive/filtered_data.csv", sep=';')
saved_imgs = np.load("/content/drive/MyDrive/images_airbnb.npy")
saved_data.shape, saved_imgs.shape

"""# Borrado de NAN de la data filtrada"""

saved_data = saved_data.dropna()

"""# Division del dataset en train y test

procedemos a dividir la información en conjuntos de entrenamiento y prueba.
"""

# Dividir los datos en conjuntos de entrenamiento y prueba
train_airbnb_df, test_valid_airbnb_df = train_test_split(saved_data, test_size=0.2, random_state=42)
test_airbnb_df, valid_airbnb_df = train_test_split(test_valid_airbnb_df, test_size=0.5, random_state=42)

# Asegurar que los tamaños de los conjuntos de prueba y validación sean iguales
test_size = len(test_airbnb_df)
valid_airbnb_df = valid_airbnb_df[:test_size]

# Guardar los archivos CSV de train, test y valid
train_airbnb_df.to_csv('./train_airbnb.csv', sep=';', decimal='.', index=False)
test_airbnb_df.to_csv('./test_airbnb.csv', sep=';', decimal='.', index=False)
valid_airbnb_df.to_csv('./valid_airbnb.csv', sep=';', decimal='.', index=False)

# Imprimir la dimensión de los dataframes
print("Dimensión del dataframe airbnb_df con el 100% de las muestras:", saved_data.shape)
print("Dimensión del dataframe train_airbnb_df con el 80% de las muestras:", train_airbnb_df.shape)
print("Dimensión del dataframe test_airbnb_df con el 25% de las muestras:", test_airbnb_df.shape)
print("Dimensión del dataframe valid_airbnb_df con el 25% de las muestras:", valid_airbnb_df.shape)

"""Validamos la creación de los archivos de train y test"""

!ls -lah

"""# Repetimos el proceso para las imagenes pero traemos los indices de los datos tabulares"""

#Guardamos imagenes train / validación / test  en ficheros
np.save('train_images_airbnb.npy',images[train_airbnb_df.index])
np.save( 'test_images_airbnb.npy',images[test_airbnb_df.index])
np.save( 'valid_images_airbnb.npy',images[valid_airbnb_df.index])

# Imprimir la dimensión de los dataframes
print("Dimensión del dataframe train_airbnb_df con el 70% de las muestras:", images[train_airbnb_df.index].shape)
print("Dimensión del dataframe test_airbnb_df con el 15% de las muestras:", images[test_airbnb_df.index].shape)
print("Dimensión del dataframe test_airbnb_df con el 15% de las muestras:", images[valid_airbnb_df.index].shape)

"""Carga de la data de train"""

# Carga de datos tabulares
df_train_airbnb   = pd.read_csv("train_airbnb.csv", sep=';')

# Carga de imagenes
images_train_airbnb = np.load('train_images_airbnb.npy')

#validamos la carga
df_train_airbnb.shape, images_train_airbnb.shape

"""# Analisis exploratorio

Descripción de las variables numéricas
"""

df_train_airbnb.describe().T

"""Hemos validado la información contenida en el dataset limpio."""

df_train_airbnb.info()

"""Ahora mostraremos aleatoriamente una selección de imágenes y sus precios correspondientes del conjunto de datos de entrenamiento de Airbnb, esto con el fin de validar que efectivamente se realizó la descarga de las imágenes y de su información asociada."""

# Seleccionar aleatoriamente un índice inicial dentro del rango del DataFrame
idx = np.random.randint(5, df_train_airbnb.shape[0]-5)

# Crear una figura y ejes con subplots
fig, axes = plt.subplots(1, 10, figsize=(15, 5))

# Iterar sobre los ejes y mostrar las imágenes y los precios correspondientes
for i, ax in enumerate(axes):
    # Verificar si todavía hay imágenes en el conjunto de datos
    if idx < images_train_airbnb.shape[0]:
        # Mostrar la imagen
        ax.imshow(images_train_airbnb[idx])
        # Establecer el título con el precio correspondiente
        ax.set_title(f'Precio = {df_train_airbnb["Price"][idx]} €')
        # Ocultar los ejes
        ax.axis('off')
        # Incrementar el índice para obtener la siguiente imagen
        idx += 1
    else:
        # Si no hay más imágenes, desactivar los ejes
        ax.axis('off')

# Ajustar el diseño y mostrar el gráfico
plt.tight_layout()
plt.show()

"""# Preprocesado

Generamos un mapa de calor para visualizar la matriz de correlación de las características del conjunto de datos de entrenamiento de Airbnb, excluyendo la variable 'Price', con el fin de identificar relaciones lineales entre las variables.
"""

# Calcular la matriz de correlación
correlation_matrix = np.abs(df_train_airbnb.drop(['Price'], axis=1).corr())
# Generar una máscara para el triángulo superior
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Corrección aquí
# Configurar la figura de matplotlib
plt.figure(figsize=(12, 10))
# Dibujar el mapa de calor con la máscara y la proporción de aspecto correcta
sns.heatmap(correlation_matrix, mask=mask, vmin=0.0, vmax=1.0, center=0.5,
            linewidths=.1, cmap="YlGnBu", cbar_kws={"shrink": .8})
plt.show()

"""# Análisis de los datos de interés a ajustar.

Price Frequency
"""

plt.figure(figsize=(15,4))
plt.subplot(1,2, 1)
df_train_airbnb['Price'].plot.hist(title='Price Frequency')
plt.show()
print("Como se puede corroborar ambas graficas confirman que la gran mayoría de los datos están distribuidos entre 20 a 100 euros. Esto nos permite definir un rango especifico análisis.")

df_train_airbnb['Price'].hist(bins=20)

"""Property type"""

plt.figure(figsize=(20,5))
plt.subplot(1,2, 1)
df_train_airbnb['Property Type'].value_counts().plot(kind='bar',title='Property Type Frequency')
plt.show()
print("La gran mayoría de los inmuebles corresponden a apartamentos")

"""Room Type"""

import matplotlib.pyplot as plt

plt.figure(figsize=(20, 5))
plt.subplot(1, 2, 2)
percentage_values = 100 * df_train_airbnb['Room Type'].value_counts() / len(df_train_airbnb['Room Type'])
counts = df_train_airbnb['Room Type'].value_counts()
bars = percentage_values.plot(kind='bar', title='% Room Type')
for i, value in enumerate(percentage_values):
    plt.text(i, value + 0.5, f'{counts.index[i]}\n{value:.2f}%', ha='center', va='bottom', fontsize=10)
plt.show()

"""Bed Type"""

import matplotlib.pyplot as plt

plt.figure(figsize=(20, 5))
plt.subplot(1, 2, 2)
percentage_values = 100 * df_train_airbnb['Bed Type'].value_counts() / len(df_train_airbnb['Room Type'])
counts = df_train_airbnb['Bed Type'].value_counts()
bars = percentage_values.plot(kind='bar', title='% Bed Type')
for i, value in enumerate(percentage_values):
    plt.text(i, value + 0.5, f'{counts.index[i]}\n{value:.2f}%', ha='center', va='bottom', fontsize=10)
plt.show()

"""Cancellation Policy"""

import matplotlib.pyplot as plt

plt.figure(figsize=(20, 5))
plt.subplot(1, 2, 2)
percentage_values = 100 * df_train_airbnb['Cancellation Policy'].value_counts() / len(df_train_airbnb['Cancellation Policy'])
counts = df_train_airbnb['Cancellation Policy'].value_counts()
bars = percentage_values.plot(kind='bar', title='% Cancellation Policy')
for i, value in enumerate(percentage_values):
    plt.text(i, value + 0.5, f'{counts.index[i]}\n{value:.2f}%', ha='center', va='bottom', fontsize=10)
plt.show()

"""Matriz de Alta Correlación."""

# Define la función para obtener columnas numéricas
def obtener_columnas_numericas(df):
    return df.select_dtypes(include=[np.number]).columns.tolist()
# Calcula la matriz de correlación
columnas_numericas = obtener_columnas_numericas(df_train_airbnb)
corr = np.abs(df_train_airbnb[columnas_numericas].drop(['Price'], axis=1).corr())
# Genera una máscara para el triángulo superior
mask = np.triu(np.ones_like(corr, dtype=bool))
# Configura la figura de matplotlib
plt.figure(figsize=(12, 10))
# Dibuja el mapa de calor con la máscara y la proporción de aspecto correcta
sns.heatmap(corr, mask=mask, vmin=0.0, vmax=1.0, center=0.5,
            linewidths=0.1, cmap="YlGnBu", cbar_kws={"shrink": 0.8})
plt.show()

"""Ahora calculamos la matriz de correlación entre las columnas numéricas del DataFrame df_train_airbnb."""

df_train_airbnb[list(df_train_airbnb.columns.values)].corr(numeric_only=True)

"""
Para obtener una visión comprensiva de las relaciones entre las variables en cuestión, realizamos un scatter plot que muestre cada variable en comparación con las demás. No obstante, debido al considerable número de variables, la distinción entre ellas se vuelve complicada.
     """

pd.plotting.scatter_matrix(df_train_airbnb, alpha=0.2, figsize=(15, 15), diagonal = 'kde')
plt.show()

"""# Resumen preprocesamiento

Se calcula la distancia en metros entre dos puntos geográficos utilizando la fórmula de la distancia haversine. La distancia se calcula a partir de las coordenadas de latitud y longitud de dos ubicaciones.
"""

def haversine_distance(lat1, lon1, lat2, lon2):
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])
    r = 6371
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2
    c = 2 * np.arcsin(np.sqrt(a))
    return c * r * 1000

useful_data = ['ID', 'Listing Url', 'Scrape ID', 'Last Scraped', 'Summary', 'Space', 'Description',
                'Neighborhood Overview', 'Notes', 'Transit', 'Access', 'Interaction', 'House Rules',
                'Medium Url', 'Picture Url', 'XL Picture Url', 'Host ID', 'Host URL',
                'Host Name', 'Host Location', 'Host Thumbnail Url', 'Host Picture Url',
                'Host Neighbourhood','City', 'Market', 'Country Code', 'Country', 'Has Availability',
                'License', 'Jurisdiction Names','Street','State','Name','Host About','First Review',
                'Last Review', 'Calendar last Scraped', 'Geolocation','Host Acceptance Rate',
                'Square Feet', 'Security Deposit', 'Amenities', 'Availability 30', 'Availability 60',
                'Availability 90', 'Availability 365','Review Scores Rating', 'Review Scores Accuracy',
                'Review Scores Cleanliness', 'Review Scores Checkin', 'Cleaning Fee', 'Host Acceptance Rate',
                'Review Scores Communication', 'Review Scores Location', 'Monthly Price',
                'Review Scores Value', 'Jurisdiction Names', 'Weekly Price','Host Response Time','Neighbourhood','Thumbnail Url'
            ]

df_train_airbnb.describe().T

df_train_airbnb.shape

def elimina_columnas(df, columnas):
    df = df.drop(columns=columnas, errors='ignore')
    return df


def prepoc_df(df, images_tot):
    # Longitud y Latitud centro Madrid para calcular la distancia haversine
    centro_madrid_lat = 40.41831
    centro_madrid_lon = -3.70275

    # Quitamos las columnas con su mayoría de variables a nulos
    df = elimina_columnas(df, useful_data)

    # Nos quedamos solo con los apartamentos
    df = df[df['Property Type'] == 'Apartment']  # Apartment

    # Room type diferente a shared
    df = df[df['Room Type'] != 'Shared room'] # Shared room

    # Bed type igual a Real bed
    df = df[df['Bed Type'] == 'Real Bed'] # Real Bed

    # Price entre 9 y 100 €
    df = df[(df['Price'] > 9) & (df['Price'] <= 100)]

    # Aplicamos distancia haversine para saber a qué distancia del centro están
    df = df.assign(centro =  lambda x: haversine_distance(x['Latitude'],x['Longitude'],centro_madrid_lat,centro_madrid_lon))

    # Eliminamos columnas las cuales hemos generado nuevas variables
    df = elimina_columnas(df, ['Longitude', 'Latitude'])

    # Se rellenan valores nulos con la media
    df.fillna(df.mean(), inplace=True)

    #Filtrar imagenes
    images_tot =  images_tot[df.index]

    return df, images_tot

#Cargamos los datos de test
test_airbnb_df  = pd.read_csv('test_airbnb.csv', sep=';', decimal='.')
images_test_airbnb  = np.load("test_images_airbnb.npy")

valid_airbnb_df  = pd.read_csv('valid_airbnb.csv', sep=';', decimal='.')
images_valid_airbnb  = np.load("valid_images_airbnb.npy")

df_train_prepoc,images_train_airbnb = prepoc_df(df_train_airbnb,images_train_airbnb)
df_test_prepoc,images_test_airbnb = prepoc_df(test_airbnb_df,images_test_airbnb)
df_valid_prepoc,images_valid_airbnb = prepoc_df(valid_airbnb_df,images_valid_airbnb)

print(f'Dimensiones del dataset de train después del preprocesamiento: {df_train_prepoc.shape}')
print(f'Dimensiones del dataset de test después del preprocesamiento: {df_test_prepoc.shape}')
print(f'Dimensiones del dataset de valid después del preprocesamiento: {df_valid_prepoc.shape}')

"""Proceso de validación"""

df_train_prepoc.describe().T

df_train_prepoc.info()

df_train_prepoc.isna().sum()

df_train_prepoc.isnull().any()

#Aplicamos la media a valores nulos
df_train_prepoc[obtener_columnas_numericas(df_train_prepoc)] = df_train_prepoc[obtener_columnas_numericas(df_train_prepoc)].fillna(df_train_prepoc[obtener_columnas_numericas(df_train_prepoc)].mean())
df_test_prepoc[obtener_columnas_numericas(df_test_prepoc)] = df_test_prepoc[obtener_columnas_numericas(df_test_prepoc)].fillna(df_train_prepoc[obtener_columnas_numericas(df_test_prepoc)].mean())


#Creación X_train , y_train , X_test e y_test
features = df_train_prepoc[obtener_columnas_numericas(df_train_prepoc)].columns.drop(['Price'])
X_train = df_train_prepoc[features].values
y_train = df_train_prepoc['Price'].values
X_test  = df_test_prepoc[features].values
y_test  = df_test_prepoc['Price'].values
X_valid  = df_test_prepoc[features].values
y_valid  = df_test_prepoc['Price'].values


#Escalamos (con los datos de train)
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
X_valid = scaler.transform(X_valid)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
print(X_valid.shape)
print(y_valid.shape)

# Convertiendo las etiquetas de clase en un formato de codificación one-hot.
y_train_cat =  to_categorical(y_train)
y_valid_cat =  to_categorical(y_valid)
y_test_cat  =  to_categorical(y_test)

"""# Modelado

El código define y entrena un modelo de red neuronal de perceptrón multicapa (MLP) utilizando TensorFlow/Keras. Este modelo tiene una arquitectura simple con una capa de entrada, dos capas ocultas y una capa de salida.

1. Optimizador:
  *  En el primer script, se utiliza un optimizador SGD (Gradiente Descendente Estocástico) con una tasa de aprendizaje fija y un término de decaimiento (decay) para ajustar la tasa de aprendizaje durante el entrenamiento.
  *  En el segundo script, se utiliza el optimizador Adam, que es un algoritmo de optimización adaptativo que ajusta automáticamente la tasa de aprendizaje.
2. Definición del modelo:
  *  En el primer script, el modelo se define secuencialmente agregando capas una por una utilizando modelMLP.add().
  *  En el segundo script, el modelo se define utilizando una lista de capas pasadas como argumento al constructor Sequential, lo que permite definir todas las capas de manera más compacta.
3. Compilación del modelo:
  *  En ambos scripts, el modelo se compila con la misma función de pérdida (mean_squared_error) y función de optimización.
4. Entrenamiento del modelo:
  *  Ambos modelos se entrenan de la misma manera utilizando los mismos datos de entrenamiento, validación y prueba, así como el mismo tamaño de lote y número de épocas.

SGD (Gradiente Descendente Estocástico)
"""

input_dim = X_train.shape[1]

modelMLP = Sequential()
modelMLP.add(Dense(8, input_dim=input_dim, activation="relu"))
modelMLP.add(Dense(4, activation="relu"))
modelMLP.add(Dense(1, activation="linear"))

epochs = 50
learning_rate = 0.01
decay= learning_rate / epochs
opt = tf.keras.optimizers.legacy.SGD(lr=learning_rate, decay=decay)
modelMLP.compile(loss="mean_squared_error", optimizer=opt)

print("[INFO] training model...")
history = modelMLP.fit(x=X_train, y=y_train_cat, shuffle=True, batch_size=32, epochs=epochs, validation_data=(X_valid, y_valid_cat), verbose=1)

# Evaluamos el modelo
loss = modelMLP.evaluate(X_test, y_test)

print("[INFO] scores...")
print(f'Loss={loss}')

print("[INFO] predicting house prices...")
preds = modelMLP.predict(X_test)
diff = preds.flatten() - y_test
percentDiff = (diff / y_test) * 100
absPercentDiff = np.abs(percentDiff)
print("[INFO] Imprimo error en porcentaje...")
print(absPercentDiff)

"""optimizador Adam"""

# input_dim = X_train.shape[1]

# modelMLP = Sequential([
#     Dense(8, input_dim=input_dim, activation="relu"),
#     Dense(4, activation="relu"),
#     Dense(1, activation="linear")
# ])

# epochs = 50
# learning_rate = 0.01
# opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# modelMLP.compile(loss="mean_squared_error", optimizer=opt)

# print("[INFO] training model...")
# history = modelMLP.fit(x=X_train, y=y_train_cat, shuffle=True, batch_size=32, epochs=epochs, validation_data=(X_valid, y_valid_cat), verbose=1)

# # Evaluamos el modelo
# loss = modelMLP.evaluate(X_test, y_test)

# print("[INFO] scores...")
# print(f'Loss={loss}')

# print("[INFO] predicting house prices...")
# preds = modelMLP.predict(X_test)
# diff = preds.flatten() - y_test
# percentDiff = (diff / y_test) * 100
# absPercentDiff = np.abs(percentDiff)
# print("[INFO] Imprimo error en porcentaje...")
# print(absPercentDiff)

"""El siguiente código compila un modelo de redes neuronales multicapa (MLP) utilizando la función de pérdida de entropía cruzada categórica, el optimizador Adam y mide la precisión como métrica. Luego, imprime un resumen del modelo, mostrando la arquitectura de las capas y el número de parámetros entrenables."""

modelMLP.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
modelMLP.summary()

plot_model(modelMLP, show_shapes=True, show_layer_names=True)

"""El código muestra un gráfico de la pérdida (loss) del modelo a lo largo de las épocas tanto para el conjunto de entrenamiento como para el conjunto de prueba."""

plt.plot(history.history['val_loss'])
plt.plot(history.history['loss'])
plt.title('Modelo')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""# Modelado Imagenes

Regresion con CNN

Este código define y entrena un modelo de red neuronal convolucional (CNN) utilizando TensorFlow y Keras. El modelo tiene tres capas convolucionales seguidas de capas de agrupación máxima (max pooling) y capas de abandono (dropout) para evitar el sobreajuste. Luego, hay una capa completamente conectada con activación ReLU y restricción de norma máxima (max_norm). Finalmente, hay una capa de salida con una neurona para la regresión lineal.
"""

# Inicializamos el modelo
modelCNN = Sequential()

# Definimos una capa convolucional
modelCNN.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))
modelCNN.add(MaxPooling2D(pool_size=(2, 2)))
modelCNN.add(Dropout(0.25))

# Definimos una segunda capa convolucional
modelCNN.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
modelCNN.add(MaxPooling2D(pool_size=(2, 2)))
modelCNN.add(Dropout(0.25))

# Definimos una tercera capa convolucional
modelCNN.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
modelCNN.add(MaxPooling2D(pool_size=(2, 2)))
modelCNN.add(Dropout(0.25))

# Añadimos nuestra "salida" con 1 neurona para la regresión lineal
modelCNN.add(Flatten())
modelCNN.add(Dense(512, activation='relu', kernel_regularizer = L1(0.01)))
modelCNN.add(Dropout(0.5))
modelCNN.add(Dense(1, activation='linear'))


# Compilamos el modelo
epochs = 50
learning_rate = 0.001
opt = Adam(learning_rate=learning_rate)
modelCNN.compile(loss="mean_squared_error", optimizer=opt)

# Entrenamos el modelo
historyCNN = modelCNN.fit(x=images_train_airbnb, y=y_train_cat, batch_size=64, shuffle=True, validation_data=(images_valid_airbnb, y_valid_cat), epochs=epochs, verbose=1)

# Evaluamos el modelo
lossCNN = modelCNN.evaluate(images_test_airbnb, y_test_cat)

plot_model(modelCNN, show_shapes=True, show_layer_names=True)

modelCNN.summary()

"""
El código proporcionado crea un gráfico que muestra la evolución de la pérdida (loss) del modelo a lo largo de las épocas durante el entrenamiento."""

plt.plot(historyCNN.history['val_loss'])
plt.plot(historyCNN.history['loss'])
plt.title('Modelo')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""# Combinación datos tabulares e imágenes

El código realiza la unificación y concatenación de las salidas de dos modelos de redes neuronales (MLP y CNN), luego obtiene la forma de la entrada combinada, divide los datos en conjuntos de entrenamiento y prueba, y finalmente imprime las formas de los conjuntos de entrenamiento y prueba junto con la forma de `y_test`.
"""

# Definir las salidas de los modelos MLP y CNN
outputMLP = modelMLP.output
outputCNN = modelCNN.output

# Definir dos conjuntos de entradas
inputShapeMLP = outputMLP.shape[1]
inputShapeCNN = outputCNN.shape[1:]

inputMLP = Input(shape=(inputShapeMLP,), name='inputMLP')  # Corregido para especificar la forma correctamente
inputCNN = Input(shape=(32, 32, 3), name='inputCNN')  # Corregido para especificar la forma correctamente

# La primera rama opera sobre la primera entrada (salida de MLP)
x = Dense(8, activation="relu")(inputMLP)
x = Dense(4, activation="relu")(x)
output_x = Dense(4, activation="relu")(x)  # Añadimos una capa de salida para 'x'
x = Model(inputs=inputMLP, outputs=output_x)

# La segunda rama opera sobre la segunda entrada (salida de CNN)
y = Conv2D(128, 3)(inputCNN)
y = Flatten()(y)
y = Dense(32, activation="relu")(y)
output_y = Dense(4, activation="relu")(y)  # Añadimos una capa de salida para 'y'
y = Model(inputs=inputCNN, outputs=output_y)

# Combinar las salidas de las dos ramas
combined = Concatenate()([output_x, output_y])  # Usamos las capas de salida 'output_x' y 'output_y'

# Aplicar una capa totalmente conectada y luego una predicción de regresión sobre las salidas combinadas
z = Dense(2, activation="relu")(combined)
z = Dense(1, activation="linear")(z)

# Nuestro modelo aceptará las entradas de las dos ramas y luego generará un único valor de salida
history = Model(inputs=[x.input, y.input], outputs=z)

plot_model(history, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

history.summary()

"""# CONCLUSIÓN

Durante el desarrollo de la práctica, se crearon tres tipos distintos de modelos para procesar los datos:

1. Modelo MLP (Perceptrón Multicapa): Este modelo se diseñó específicamente para procesar datos tabulares. Se trata de un modelo secuencial, lo que implica que las capas están apiladas secuencialmente una detrás de otra. Este modelo se desarrolló como un modelo de regresión simple, capaz de tomar una entrada y generar una salida numérica.

2. Modelo CNN (Red Neuronal Convolucional): Para el procesamiento de imágenes, se implementó un modelo de red neuronal convolucional. Este modelo sigue una arquitectura secuencial, con capas convolucionales intercaladas con capas de agrupación máxima (max pooling) y capas de abandono (dropout). La salida resultante se aplana y se conecta a través de capas densas para producir la salida final.

3. Modelo Híbrido: Se desarrolló un modelo híbrido que combina información tanto del modelo MLP como del modelo CNN. Esta combinación de ramas permite que el modelo aprenda patrones de datos tanto estructurados como de imágenes, lo que resulta útil en problemas que requieren el análisis de múltiples tipos de datos.

Al analizar los gráficos de los modelos MLP y CNN, se observa que las curvas de testing siguen de cerca el comportamiento de las curvas de training. Este fenómeno sugiere que los modelos individuales fueron entrenados de manera adecuada y no muestran signos de sobreajuste o subajuste en los datos de testing. Es razonable esperar que el modelo híbrido, construido a partir de las salidas de los modelos individuales (MLP y CNN), también presente un comportamiento similar en términos de ajuste durante el entrenamiento y la evaluación.

Sin embargo, es importante mencionar que, para los propósitos de esta práctica, no se llevó a cabo una evaluación y validación específica del rendimiento de los modelos híbrido, MLP y CNN. Más allá de los análisis presentados a lo largo de este documento, no se realizaron pruebas exhaustivas para evaluar el rendimiento individual de cada modelo en términos de métricas específicas como precisión, recall, F1-score, entre otras.
"""